\section{Introduction}
The promise of deep learning is to discover rich, hierarchical models \cite{2_MAL-006} that represent probability distributions over the kinds of data encountered in artificial intelligence applications, such as natural images, audio waveforms containing speech, and symbols in natural language corpora. So far, the most striking successes in deep learning have involved discriminative models, usually those that map a high-dimensional, rich sensory input to a class label \cite{14_6296526, 22_NIPS2012_c399862d}. These striking successes have primarily been based on the backpropagation and dropout algorithms, using piecewise linear units \cite{19_5459469,9_pmlr-v15-glorot11a,10_goodfellow2013maxoutnetworks} which have a particularly well-behaved gradient. Deep \emph{generative} models have had less of an impact, due to the difficulty of approximating many intractable probabilistic computations that arise in maximum likelihood estimation and related strategies, and due to difficulty of leveraging the benefits of piecewise linear nits in the generative context. We propose a new generative model estimation procedure that sidesteps these difficulties.
\footnote{All code and hyperparameters available at \url{http://www.github.com/goodfeli/adversarial}}

\noindent In the proposed \emph{adversarial nets} framework, the generative model is pitted against an adversary: a discriminate model that leans to determine whether a sample is from the model distribution or the data distribution. The generative model can be thought of as analogous to a team of counterfeiters, trying to produce fake currency and use it without detection, while the discriminative model is analogous to the police, trying to detect the counterfeit currency. Competition in this game drives both teams to improve their methods until the counterfeits are indistinguishable
from the genuine articles.

\noindent This framework can yield specific training algorithms for many kinds of model and optimization algorithm. In this article, we explore the special case when the generative model generates sample by passing random noise through a multilayer perceptron, and the discriminative model is also a multilayer perceptron. We refer to this special case as \emph{adversarial nets}. In this case, we can train both models using only the highly successful backpropagation and dropout algorithms \cite{17_hinton2012improvingneuralnetworkspreventing} and sample from the generative model using only forward propagation. No approximate inference or Markov chains are necessary.
