\section{Adversarial nets}

The adversarial modeling framework is most straightforward to apply when the models are both
multilayer perceptrons. To learn the generatorâ€™s distribution $p_g$ over data $\bx$, we define a prior on input noise variables $p_{\bz}(\bz)$, then represent a mapping to data space as $G(\bz, \theta_g)$, where $G$ is a differentiable function represented by a multilayer perceptron with parameters $\theta_g$. We also define a second multilayer perceptron $D(\bx, \theta_d)$ that outputs a single scalar. $D(\bx)$ represents the probability that $\bx$ came from the data rather than $p_g$. We train $D$ to maximize the probability of assigning the correct labe to both training examples and samples from $G$. We simultaneously train $G$ to minimize $\log(1-D(G(\bz)))$:
In other words, $D$ and $G$ play the following two-player minimax game with value function $V(F,D)$:

\begin{equation}
	\underset{G}{\min} \underset{D}{\max} V(D,G) = \mathbb{E}_{\bx\sim \pdatx}[\log D(\bx)]+\mathbb{E}_{\bz\sim p_{\bz}(\bz)}[\lomdgz].\label{eq: equation 1}
\end{equation}

In the next section, we present a theoretical analysis of adversarial nets, essentially showing that the training criterion allows one to recover the data generating distribution as $G$ and $D$ are given enough capacity, i.e., in the non-parametric limit. See % TODO: Figure 1 reference.
for a less formal, more pedagogical explanation of the approach. In practice, we must implement the game using an iterative, numerical approach. Optimizing $D$ to completion in the inner loop of training is computationally prohibitive, and on finite datasets would result in overfitting. Instead, we alternate between $k$ steps of optimizing $D$ and one step of optimizing $G$. This results in $D$ being maintained near its optimal solution, so long as $G$ changes slowly enough. This strategy is analogous to the way that SML/PCD \cite{31_younes1999convergence,29_10.1145/1390156.1390290} training maintains samples from a Markov chain from one learning step to the next in order to avoid burning in a Markov chain as part of the inner loop of learning. The procedure is formally presented in % TODO: Algorithm 1 reference\

\noindent In practice equation \ref{eq: equation 1} % TODO: Maybe use eqref.
may not provide sufficient gradient for $G$ to learn well. Early in learning, when $G$ is poor, $D$ can reject samples with high confidence because they are clearly different from the training data. In this case, $\lomdgz$ saturates. Rather than training $G$ to minimize $\lomdgz$ we can train $G$ to maximize $\log D(G(\bz))$. This objective function results in the same fixed point of the dynamics of $G$ and $D$ but provides much stronger gradients early in learning.

% TODO: Figure 1